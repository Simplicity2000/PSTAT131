---
title: "hw5"
output: html_document
date: "2022-11-20"
---

```{r}
#install.packages("janitor")
#install.packages("glmnet")

library(tidyverse)
library(tidymodels)
library(janitor)
library(dplyr)
library(ggplot2)
library(glmnet)
library(yardstick)
pkm <- read.csv(file = "C:/ucsb 2022/2022Fall/131/homework-5/homework-5/data/Pokemon.csv")
```

### Exercise 1

```{r}
pkm_cnames <- clean_names(pkm)
# It gives a tidier way to display for the variable names by lowering the case of the first letter and turning the dot or dots to underscore. It makes easy for us to track the variable names as well as avoiding error in the later syntax.
```

### Exercise 2

```{r}
barplot(table(pkm_cnames$type_1), las = 3)
# There are 18 classes. Flying class has very few Pokemon.
pkm_1 <- pkm_cnames %>%
  filter(type_1 %in% c('Bug', 'Fire', 'Grass', 'Normal', 'Water', 'Psychic'))
pkm_1$type_1 <- as.factor(pkm_1$type_1)
pkm_1$legendary <- as.factor(pkm_1$legendary)

```


### Exercise 3

```{r}
set.seed(1012)

pkm_split <- initial_split(pkm_1, prop = 0.80, strata = type_1)
pkm_train <- training(pkm_split)
pkm_test <- testing(pkm_split)

dim(pkm_train)
dim(pkm_test)
# We have enough data in training set and in testing set.

pkm_fold <- vfold_cv(pkm_train, v = 5, strata = type_1)

# It will maintain the same class ratio throughout the process, and it is good on a limited data sample.
```

### Exercise 4

```{r}
pkm_recipe <- recipe(type_1 ~ legendary+generation+sp_atk+attack+speed+defense+hp+sp_def, data = pkm_train)%>%
  step_dummy(legendary,generation)%>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())
```

### Exercise 5

```{r}
pkm_spec <- 
  multinom_reg(penalty = tune(), mixture = tune()) %>% 
  set_mode("classification")%>%
  set_engine("glmnet")

pkm_workflow <- workflow() %>% 
  add_recipe(pkm_recipe) %>% 
  add_model(pkm_spec)


net_grid <- grid_regular(penalty(range = c(-5, 5)), mixture(range = c(0,1)),levels = 10)


# There will be 500 models in total.
```

### Exercise 6

```{r}
tune_res <- tune_grid(
  pkm_workflow,
  resamples = pkm_fold, 
  grid = net_grid
)

autoplot(tune_res)
# Smaller value of penalty and mixture produce better accurary and roc_auc.
```

### Exercise 7

```{r}
best_penalty <- select_best(tune_res, metric = "roc_auc")
best_penalty


pkm_final <- finalize_workflow(pkm_workflow, best_penalty)
pkm_final_fit <- fit(pkm_final, data = pkm_train)
augment(pkm_final_fit, new_data = pkm_test) %>%
  select(type_1,starts_with(".pred"))
```

### Exercise 8

```{r}
augment(pkm_final_fit, new_data = pkm_test) %>%
  roc_auc(type_1, .pred_Bug:.pred_Water)

augment(pkm_final_fit, new_data = pkm_test) %>%
  roc_curve(type_1, .pred_Bug:.pred_Water) %>%
  autoplot()

augment(pkm_final_fit, new_data = pkm_test) %>%
  conf_mat(type_1, .pred_class) %>%
  autoplot("heatmap")
# We have an overall ROC AUC of 0.68 which is not good enough. We can see from the plot that Normal Pokemon has the best ROC curve. Our model is not good enough to beat 0.8 on testing data. Water is the model best at predicting, and grass is the worst. It may related to the sample size of each type, so we have the most water type to predict the best and the least grass type to predict the worst.
```





